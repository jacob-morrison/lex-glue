gantry run --beaker-image 'ai2/pytorch1.13.0-cuda11.6-python3.9' --workspace ai2/robustness-interpolations --cluster ai2/general-cirrascale \\
    --conda environment.yml --gpus 1 --priority high \\
    --dataset 'merge-t5-xl-sa-low:/t5-model' \\
    --dataset 'robustness-3-domains:/robustness-data' \\
    -- python -u main.py

# case hold
GPU_NUMBER=0
MODEL_NAME='bert-base-uncased'
BATCH_SIZE=8
ACCUMULATION_STEPS=1
TASK='case_hold'

CUDA_VISIBLE_DEVICES=${GPU_NUMBER} python experiments/case_hold.py --task_name ${TASK} --model_name_or_path ${MODEL_NAME} --output_dir logs/${TASK}/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir --load_best_model_at_end --metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy epoch --save_strategy epoch --save_total_limit 5 --num_train_epochs 20 --learning_rate 3e-5 --per_device_train_batch_size ${BATCH_SIZE} --per_device_eval_batch_size ${BATCH_SIZE} --seed 1 --fp16 --fp16_full_eval --gradient_accumulation_steps ${ACCUMULATION_STEPS} --eval_accumulation_steps ${ACCUMULATION_STEPS}

# ecthr a
GPU_NUMBER=0
MODEL_NAME='bert-base-uncased'
LOWER_CASE='True'
BATCH_SIZE=2
ACCUMULATION_STEPS=4
TASK='ecthr_a'

CUDA_VISIBLE_DEVICES=${GPU_NUMBER} python experiments/ecthr.py --model_name_or_path ${MODEL_NAME} --do_lower_case ${LOWER_CASE} --task ${TASK} --output_dir logs/${TASK}/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir --load_best_model_at_end --metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy epoch --save_strategy epoch --save_total_limit 5 --num_train_epochs 20 --learning_rate 3e-5 --per_device_train_batch_size ${BATCH_SIZE} --per_device_eval_batch_size ${BATCH_SIZE} --seed 1 --fp16 --fp16_full_eval --gradient_accumulation_steps ${ACCUMULATION_STEPS} --eval_accumulation_steps ${ACCUMULATION_STEPS}

# ecthr b??

# eurlex
GPU_NUMBER=6
MODEL_NAME='bert-base-uncased'
LOWER_CASE='True'
BATCH_SIZE=8
ACCUMULATION_STEPS=1
TASK='eurlex'

CUDA_VISIBLE_DEVICES=${GPU_NUMBER} python experiments/eurlex.py --model_name_or_path ${MODEL_NAME} --do_lower_case ${LOWER_CASE}  --output_dir logs/${TASK}/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir --load_best_model_at_end --metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy epoch --save_strategy epoch --save_total_limit 5 --num_train_epochs 2 --learning_rate 3e-5 --per_device_train_batch_size ${BATCH_SIZE} --per_device_eval_batch_size ${BATCH_SIZE} --seed 1 --fp16 --fp16_full_eval --gradient_accumulation_steps ${ACCUMULATION_STEPS} --eval_accumulation_steps ${ACCUMULATION_STEPS}

# ledgar
GPU_NUMBER=0
MODEL_NAME='bert-base-uncased'
LOWER_CASE='True'
BATCH_SIZE=8
ACCUMULATION_STEPS=1
TASK='ledgar'

CUDA_VISIBLE_DEVICES=${GPU_NUMBER} python experiments/ledgar.py --model_name_or_path ${MODEL_NAME} --do_lower_case ${LOWER_CASE}  --output_dir logs/${TASK}/${MODEL_NAME}/seed_1 --do_train --do_eval --do_pred --overwrite_output_dir --load_best_model_at_end --metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy epoch --save_strategy epoch --save_total_limit 5 --num_train_epochs 20 --learning_rate 3e-5 --per_device_train_batch_size ${BATCH_SIZE} --per_device_eval_batch_size ${BATCH_SIZE} --seed 1 --fp16 --fp16_full_eval --gradient_accumulation_steps ${ACCUMULATION_STEPS} --eval_accumulation_steps ${ACCUMULATION_STEPS}

# scotus
GPU_NUMBER=0
MODEL_NAME='bert-base-uncased'
LOWER_CASE='True'
BATCH_SIZE=2
ACCUMULATION_STEPS=4
TASK='scotus'

gantry run --beaker-image 'ai2/pytorch1.13.0-cuda11.6-python3.9' \
--workspace ai2/robustness-interpolations --cluster ai2/allennlp-cirrascale \
--pip requirements.txt --gpus 1 --priority high \
-- python -u experiments/scotus.py --model_name_or_path ${MODEL_NAME} \
--do_lower_case True  --output_dir logs/scotus/${MODEL_NAME}/seed_1 \
--do_train --do_eval --do_pred --overwrite_output_dir --load_best_model_at_end \
--metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy \
epoch --save_strategy epoch --save_total_limit 5 --num_train_epochs 20 \
--learning_rate 3e-5 --per_device_train_batch_size 2 \
--per_device_eval_batch_size 2 --seed 1 --fp16 --fp16_full_eval \
--gradient_accumulation_steps 4 --eval_accumulation_steps 4



# unfair tos
GPU_NUMBER=0
MODEL_NAME='bert-base-uncased'
LOWER_CASE='True'
BATCH_SIZE=8
ACCUMULATION_STEPS=1
TASK='unfair_tos'

gantry run --beaker-image 'ai2/pytorch1.13.0-cuda11.6-python3.9' \
--workspace ai2/robustness-interpolations --cluster ai2/allennlp-cirrascale \
--pip requirements.txt --gpus 1 --priority high \
-- python -u experiments/unfair_tos.py --model_name_or_path ${MODEL_NAME} \
--do_lower_case True --output_dir /results/logs/unfair_tos/${MODEL_NAME}/seed_1 \
--do_train --do_eval --do_pred --overwrite_output_dir --load_best_model_at_end \
--metric_for_best_model micro-f1 --greater_is_better True --evaluation_strategy \
epoch --save_strategy epoch --save_total_limit 5 --num_train_epochs 20 \
--learning_rate 3e-5 --per_device_train_batch_size 8 \
--per_device_eval_batch_size 8 --seed 1 --fp16 --fp16_full_eval \
--gradient_accumulation_steps 1 --eval_accumulation_steps 1
